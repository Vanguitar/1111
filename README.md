## One-dimensional latent diffusion with health-guided cross-attention mechanism

In the reverse diffusion process, a one-dimensional U-Net architecture is employed as the denoising network backbone. The network comprises three hierarchical levels with progressive downsampling and upsampling operations. At each level, self-attention mechanisms enable the network to capture long-range temporal dependencies within fault features. Crucially, cross-attention blocks are selectively integrated into the higher resolution levels of both the encoding and decoding paths.

The core innovation lies in the health-guided cross-attention mechanism, which injects domain-specific condition information into the feature synthesis process. During the training phase, fused healthy features from the source domain are used as conditioning information within the cross-attention modules. These healthy features contain the specific perturbations and impulses induced by the source operating condition, such as vibration signatures due to particular load levels or rotational speeds. The cross-attention mechanism computes attention weights that link the noisy fault features (query) to the healthy condition features (key and value), enabling the model to learn how condition-specific perturbations should influence the denoising process.

Mathematically, the cross-attention operation is formulated as:
Attention(Q, K, V) = softmax(Q K^T / √d_k) V

where Q represents the query vectors derived from the current noisy features, K and V are the key and value vectors computed from the conditioning healthy features, and d_k denotes the dimension of each attention head. The normalization by √d_k ensures numerical stability. The resulting attention-weighted combination allows the model to selectively emphasize those aspects of the source condition that are most relevant to preserving domain-specific characteristics during feature generation.

During the generation phase, the same cross-attention mechanism operates with different conditioning inputs. Instead of source domain healthy features, the model receives the difference between target domain and source domain healthy features. This delta conditioning approach enables the network to learn and apply domain-specific transformations that adapt synthesized features toward the target operating condition. The mechanism effectively captures what distinguishes the target domain from the source, guiding the synthesis toward target-domain characteristic fault features without requiring access to target-domain fault training data.

The selective placement of cross-attention modules is strategic. They are applied only at higher resolution levels of the network, where fine-grained temporal patterns are preserved. Lower resolution levels, which encode more abstract structural information, bypass cross-attention to focus on general fault characteristics that generalize across domains. This design prevents over-conditioning on domain-specific details while maintaining sufficient guidance to capture operating-condition-induced variations.


## Feature generation with Fault-Prior Self-Attention mechanism

The feature generation stage begins with random Gaussian noise, which is iteratively refined through the learned reverse diffusion process. To guide this synthesis toward fault-specific representations, a fault-prior self-attention mechanism is introduced, which constrains the generation to preserve fault-type characteristics throughout the denoising trajectory.

The self-attention blocks embedded within the U-Net architecture compute attention weights exclusively within the current feature representation. Unlike cross-attention which introduces external conditioning, self-attention enables the network to identify and reinforce internal structural patterns that distinguish different fault types. During generation, source-domain fault features serve as implicit prior constraints. The model has learned, through training on source fault data, which temporal and spectral patterns are inherent to each fault type, and this knowledge is encoded in the network weights and attention patterns.

The fault-prior self-attention mechanism operates through progressive refinement across diffusion steps. In early denoising steps, when noise is still substantial, the self-attention mechanism broadly identifies major structural features. As the process continues toward later steps, self-attention becomes increasingly fine-grained, refining subtle temporal patterns that distinguish fault types and severity levels. The mechanism prevents the generation process from drifting toward random features or misclassified fault types through step-by-step structural consistency enforcement.

Specifically, the forward process of generation proceeds as follows. Starting from Gaussian random noise x_T at timestep T, the model applies the reverse diffusion process for T steps. At each step t, the network computes a prediction of the noise component ε_t using both self-attention (which constrains fault-type consistency) and the timestep embedding (which provides the denoising guidance for the current step). The prediction is used to refine the feature according to the equation:

x_{t-1} = (1/√α_t)[x_t - ((1-α_t)/√(1-ᾱ_t)) ε_θ(x_t, t, c)]

where α_t is the noise schedule coefficient, ᾱ_t is its cumulative product, ε_θ denotes the network prediction, and c represents the conditioning information (including the delta between target and source domain health features).

The self-attention mechanism is crucial for maintaining fault-type coherence. During training on source fault data, the network learns distinctive attention patterns for each fault type. These patterns capture which features correlate with specific fault classes. During generation, the same attention mechanisms reactivate these learned patterns, guiding the synthesis to produce features with characteristic fault signatures. For instance, ball fault features possess different spectral-temporal patterns compared to inner race faults, and the self-attention mechanism preserves these distinctions by preferentially amplifying fault-type-specific feature correlations.

The combination of self-attention for fault-type constraint and cross-attention for domain adaptation creates a balanced synthesis process. Cross-attention ensures that generated features align with the target operating condition, while self-attention ensures that despite condition changes, the fundamental fault characteristics are preserved. This dual-guidance strategy prevents feature collapse (where all features converge to averaging effects) and ensures that synthesized features remain fault-discriminative across domain shifts. The quality of generated features is maintained across the entire denoising trajectory through this integrated mechanism.


## Classifier generalization and novel domain diagnosis

Following feature generation, the synthesized target-domain fault features are combined with source-domain features to train a unified classifier. The training dataset consists of both the original source-domain features and the newly synthesized target-domain features, with balanced representation across all fault classes and severity levels.

A standard deep learning classifier, such as a convolutional neural network or fully connected network, is trained on this augmented dataset using cross-entropy loss. The inclusion of synthesized target-domain features exposes the classifier to examples that exhibit the statistical characteristics of the target domain while maintaining fault-class distinctions learned from the source domain. During testing on unseen target-domain samples, the classifier makes predictions based on the learned decision boundaries that have been informed by both source and generated target features.

The effectiveness of this approach depends critically on the quality and fidelity of synthesized features. The evaluation results presented in Section 4 demonstrate that synthesized features maintain strong consistency across multiple statistical metrics (Euclidean distance, Pearson correlation, MSE, and MAE), indicating that they capture both the magnitude and temporal structure of target-domain fault features. This consistency ensures that the classifier trained on synthesized features generalizes reliably to actual target-domain test samples.
